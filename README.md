Train & Chat w/ ~135m Parameter models on local GPU.

Chat with any Huggingface LLM via Python on your local machine, with back-and-forth entries.  This is optimized for super small LMs, such as SMOLLM (135m Parameters).
Can easily run this on a local GPU.

This uses SFT, LoRA to fine-tune a model directly from Huggingface, but the requirements.txt allows for other types of fine-tuning.
